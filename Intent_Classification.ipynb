{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "with open(\"./oos-eval/data/data_full.json\") as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "labels = [ d[1] for d in data['train']] #Get all the labels\n",
    "\n",
    "# For persistence\n",
    "np.random.seed(0)\n",
    "\n",
    "uniqueLabels = np.unique(np.array(labels))\n",
    "\n",
    "# Labels of interest- 20 selected at random\n",
    "finalLabels = list(np.random.choice(uniqueLabels,20,replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into train, val and test\n",
    "\n",
    "splits = [\"train\",\"val\",\"test\"]\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "val_x = []\n",
    "val_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "dataset = {\n",
    "    \"train\": [train_x, train_y],\n",
    "    \"val\": [val_x, val_y],\n",
    "    \"test\" : [test_x, test_y]\n",
    "}\n",
    "\n",
    "for split in splits:\n",
    "  np.random.shuffle(data[split])\n",
    "  for d in data[split]:\n",
    "    if d[1] in finalLabels:\n",
    "      dataset[split][0].append(d[0])\n",
    "      dataset[split][1].append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding - from labels to classes\n",
    "\n",
    "labelToClsIdx = {}\n",
    "for idx, label in enumerate(finalLabels):\n",
    "  labelToClsIdx[label] = idx\n",
    "#print(labelToClsIdx)\n",
    "\n",
    "for split in splits:\n",
    "  classIds = [ labelToClsIdx[label] for label in dataset[split][1]]\n",
    "  dataset[split].append(classIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n  def load_model(self, path):\\n    checkpoint = torch.load(path)\\n    self.ffn.load_state_dict(checkpoint['ffn_dict'])\\n    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "#print(device)\n",
    "\n",
    "class BERTClassifier():\n",
    "    \n",
    "    def __init__(self, dataset, model_name=\"distilbert-base-uncased\", n_classes=20):\n",
    "    \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "        # get the sentences and class ids\n",
    "        self.train_x = dataset[\"train\"][0]\n",
    "        self.train_y = dataset[\"train\"][2]\n",
    "        self.val_x = dataset[\"val\"][0]\n",
    "        self.val_y = dataset[\"val\"][2]\n",
    "        self.test_x = dataset[\"test\"][0]\n",
    "        self.test_y = dataset[\"test\"][2]\n",
    "\n",
    "        # Feed Forward Network \n",
    "        self.ffn = nn.Linear(768, n_classes).to(device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.ffn.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "    def tokenize(self, sequences):\n",
    "    \n",
    "        bert_encoding = self.tokenizer(sequences, return_tensors=\"pt\", padding=\"longest\", truncation=True)\n",
    "        input_ids = bert_encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = bert_encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "\n",
    "    def get_embeddings(self, sequences):\n",
    "        \n",
    "        inp_ids, a_masks = self.tokenize(sequences)\n",
    "        outputs = self.model(inp_ids, attention_mask= a_masks, output_hidden_states=True)\n",
    "        seq_embedding = outputs.hidden_states[-1][:,0]\n",
    "\n",
    "        return seq_embedding\n",
    "\n",
    "  \n",
    "    def train(self, epochs =25, lr = 1e-4, batch_size=32):\n",
    "    \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        best_acc = 0\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "        for ep in range(epochs):\n",
    "          print(f\"\\nEpoch {ep+1}\")\n",
    "          tr_loss = []\n",
    "\n",
    "          for batch_idx in range(0,len(self.train_x), batch_size):\n",
    "            self.optimizer.zero_grad()\n",
    "            x,y = (list(self.train_x[batch_idx : batch_idx+batch_size]),\n",
    "                   list(self.train_y[batch_idx : batch_idx+batch_size]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "              emb_x = self.get_embeddings(x)\n",
    "\n",
    "            logits = self.ffn(emb_x)\n",
    "            loss = loss_fn(logits, torch.LongTensor(y).to(device))\n",
    "\n",
    "            if device == \"cuda\":\n",
    "              tr_loss.append(loss.detach().cpu().numpy())\n",
    "            else:\n",
    "              tr_loss.append(loss.detach().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "          print(\"Training Loss: {}\".format(np.array(tr_loss).mean()))\n",
    "\n",
    "          dev_acc = self.evaluate()\n",
    "\n",
    "          print(\"Dev Accuracy: {}\".format(dev_acc))\n",
    "\n",
    "          if dev_acc > best_acc:\n",
    "            best_acc = dev_acc \n",
    "            self.best_ffn = self.ffn\n",
    "\n",
    "            torch.save({\"epoch\": ep, \n",
    "                        \"ffn_dict\": self.ffn.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'accuracy': dev_acc}, f\"./model_{ep+1}_{dev_acc}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    def evaluate(self, split=\"val\", batch_size=32, use_best_ffn=False):\n",
    "        \n",
    "        if split == \"val\":\n",
    "          data_x = self.val_x\n",
    "          data_y = self.val_y\n",
    "        else:\n",
    "          data_x = self.test_x\n",
    "          data_y = self.test_y   \n",
    "\n",
    "        preds = []\n",
    "        target = []\n",
    "\n",
    "        for batch_idx in range(0,len(data_x), batch_size):\n",
    "\n",
    "            x,y = (list(data_x[batch_idx : batch_idx+batch_size]),\n",
    "                   list(data_y[batch_idx : batch_idx+batch_size]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "              emb_x = self.get_embeddings(x)\n",
    "              if use_best_ffn:\n",
    "                logits = self.best_ffn(emb_x)\n",
    "              else:\n",
    "                logits = self.ffn(emb_x)\n",
    "              pred = torch.argmax(logits,dim=1)\n",
    "\n",
    "              target.extend(y)\n",
    "              if device == \"cuda\":\n",
    "                preds.extend(list(pred.detach().cpu().numpy()))\n",
    "              else:\n",
    "                preds.extend(list(pred.detach().numpy()))\n",
    "\n",
    "        acc = accuracy_score(np.array(target), np.array(preds))\n",
    "\n",
    "        return acc\n",
    "        \n",
    "\n",
    "    def load_model(self, path):\n",
    "        \n",
    "        checkpoint = torch.load(path)\n",
    "        self.ffn.load_state_dict(checkpoint['ffn_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training Loss: 2.766836404800415\n",
      "Dev Accuracy: 0.7\n",
      "\n",
      "Epoch 2\n",
      "Training Loss: 2.308286190032959\n",
      "Dev Accuracy: 0.86\n",
      "\n",
      "Epoch 3\n",
      "Training Loss: 1.9314401149749756\n",
      "Dev Accuracy: 0.885\n",
      "\n",
      "Epoch 4\n",
      "Training Loss: 1.6273757219314575\n",
      "Dev Accuracy: 0.8875\n",
      "\n",
      "Epoch 5\n",
      "Training Loss: 1.3855042457580566\n",
      "Dev Accuracy: 0.905\n",
      "\n",
      "Epoch 6\n",
      "Training Loss: 1.1936445236206055\n",
      "Dev Accuracy: 0.91\n",
      "\n",
      "Epoch 7\n",
      "Training Loss: 1.0406631231307983\n",
      "Dev Accuracy: 0.9225\n",
      "\n",
      "Epoch 8\n",
      "Training Loss: 0.9174762964248657\n",
      "Dev Accuracy: 0.9275\n",
      "\n",
      "Epoch 9\n",
      "Training Loss: 0.8170788884162903\n",
      "Dev Accuracy: 0.93\n",
      "\n",
      "Epoch 10\n",
      "Training Loss: 0.7342090010643005\n",
      "Dev Accuracy: 0.9325\n",
      "\n",
      "Best Dev Accuracy: 0.9325\n",
      "Test Accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "#Run the model\n",
    "\n",
    "bert_model = BERTClassifier(dataset)\n",
    "bert_model.train(epochs =10, lr = 5e-4)\n",
    "test_acc = bert_model.evaluate(split=\"test\",use_best_ffn=False)\n",
    "best_dev_acc = bert_model.evaluate(split=\"val\",use_best_ffn=False)\n",
    "\n",
    "print(\"\\nBest Dev Accuracy: {}\".format(best_dev_acc))\n",
    "print(\"Test Accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
